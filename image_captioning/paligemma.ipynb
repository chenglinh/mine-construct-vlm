{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clhsieh/miniconda3/envs/minecraft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from transformers.image_utils import load_image\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_text(fpaths, by_lines=False):\n",
    "    with open(fpaths, \"r\") as fp:\n",
    "        if by_lines:\n",
    "            return fp.readlines()\n",
    "        else:\n",
    "            return fp.read()\n",
    "\n",
    "def load_prompt(prompt):\n",
    "    return load_text(f\"prompts/{prompt}.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.82s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/paligemma2-10b-mix-448\"\n",
    "# default: Load the model on the available device(s)\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(f\"/nfs/turbo/coe-stellayu/clhsieh/Minecraft/ckpt/{model_name}\", torch_dtype=torch.bfloat16, device_map=\"auto\").eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(f\"{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and define task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for image_name in ['easy-0.png']: # ['easy-1.png', 'easy-2.png', 'easy-3.png', 'easy-4.png', 'easy-5.png']: # ['barn_house.webp', 'castle_wall.webp', 'greek_house.webp', 'mg_nest.png', 'japanese_house.webp', 'incinerator.png']:\n",
    "# image_name = 'greek_house.webp' # barn_house.webp, castle_wall.webp, greek_house.webp, mg_nest.png, japanese_house.webp, incinerator.png\n",
    "    # image_name = f'easy-{i}.png'\n",
    "    folder = 'easy'\n",
    "    img_path = f\"/nfs/turbo/coe-stellayu/clhsieh/Minecraft/data/{folder}/{image_name}\"\n",
    "    image_name_without_ext = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    output_dir = f\"output/{model_name}/{folder}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = f\"{output_dir}/{image_name_without_ext}.txt\"\n",
    "    \n",
    "\n",
    "    # building_description_system = load_prompt(\"building_description_system\")\n",
    "    prompt = load_prompt(\"building_description_query\")\n",
    "    # prompt = \"answer en description of the image\"\n",
    "    raw_image = Image.open(img_path)\n",
    "    # image = load_image(img_path)\n",
    "    prompt = \"<image>\" + prompt\n",
    "    model_inputs = processor(text=prompt, images=raw_image,\n",
    "                  padding=\"longest\", do_convert_rgb=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "    # prompt = \"\"\n",
    "    # model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**model_inputs, max_new_tokens=4000, do_sample=False)\n",
    "        generation = generation[0][input_len:]\n",
    "        output_text = processor.decode(generation, skip_special_tokens=True)\n",
    "        print(output_text)\n",
    "\n",
    "    # Save the output_text as a txt file with the name of image_name without the file type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the output_text to the file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 3\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[43moutput_text\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Display the image\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# Save the output_text to the file\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    file.write(output_text[0])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Display the image\n",
    "display(Image(filename=img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
